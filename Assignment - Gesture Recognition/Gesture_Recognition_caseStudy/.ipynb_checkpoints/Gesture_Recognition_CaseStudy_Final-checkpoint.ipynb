{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import cv2 # install using `pip install opencv-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/project/PROJECT\n"
     ]
    }
   ],
   "source": [
    "print(os. getcwd())\n",
    "\n",
    "root_path = '/mnt/disks/user/project/PROJECT/Project_data' #chnage this as necessary\n",
    "\n",
    "train_doc = np.random.permutation(open(root_path+'/train.csv').readlines()) #path to train.doc change only if the file is not in the same folder\n",
    "val_doc = np.random.permutation(open(root_path+'/val.csv').readlines()) #path to train.doc change only if the file is not in the same folder\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 120   # no of pixels in x -- here its interprted as z in the starter code\n",
    "n_cols = 120   # no of pixels in y -- here its interprted as y in the starter code\n",
    "n_frames = 30  # no of frames in the video/ z dimension -- here its interprted as x in the starter code\n",
    "n_channel = 3  # as the files are RGB there are 3 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getBatchData will load the images from directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatchData(source_path, folder_list, batch, batch_size,t):\n",
    "    img_idx = [x for x in range(1,30,30//n_frames)]\n",
    "    batch_data = np.zeros((batch_size,n_frames,n_cols,n_rows,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "            image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "\n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "            if image.shape[0] != image.shape[1]:\n",
    "                image=image[0:120,20:140]\n",
    "            resized_image = cv2.resize(image, (n_rows,n_cols), interpolation = cv2.INTER_AREA)\n",
    "            #setting the resized data and the corresponding lables.\n",
    "            batch_data[folder,idx] = (resized_image)\n",
    "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "    return batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator function is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size ,n_frames=30):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            yield getBatchData(source_path, folder_list, batch, batch_size,t) #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            yield getBatchData(source_path, folder_list, batch, batch_size, t)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Printing the paths and the number of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = root_path+r'/train'\n",
    "val_path = root_path+'/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20\n",
    "print ('# epochs =', num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,  Flatten, BatchNormalization, Activation, Dropout#,GRU, TimeDistributed, \n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the size of filters for Convolution layers and the number of neurons in the Dense layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = [8,16,32,64]\n",
    "n_dense = [256, 128, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Shape of the Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(n_frames,n_rows,n_cols,n_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the **Convolution 3D** Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model With 4 Conv3D layers and No Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "#Adding first layer of CONV3D\n",
    "model_1.add(Conv3D(n_filters[0], kernel_size=(3,3,3), input_shape=input_shape,padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "#Adding second layer of CONV3D\n",
    "model_1.add(Conv3D(n_filters[1], kernel_size=(3,3,3), padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "#Adding third layer of CONV3D\n",
    "model_1.add(Conv3D(n_filters[2], kernel_size=(1,3,3), padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "#Adding fourth layer of CONV3D\n",
    "model_1.add(Conv3D(n_filters[3], kernel_size=(1,3,3), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model_1.add(Flatten())\n",
    "\n",
    "#Adding Dense Layers without DropOuts:\n",
    "model_1.add(Dense(n_dense[0], activation='relu'))\n",
    "model_1.add(Dense(n_dense[1], activation='relu'))\n",
    "\n",
    "#softmax layer\n",
    "model_1.add(Dense(n_dense[2], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ADAM optimizer, and Cross Entropy loss, while choosing the metric to be categorical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Train and Validation Generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting steps_per_epoch and validation_steps which will be used by fit_generator, to decide the number of next calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init_1' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/val ; batch size = 16\n",
      "Source path =  /mnt/disks/user/project/PROJECT/Project_data/train ; batch size =Epoch 1/20 16\n",
      "\n",
      "42/42 [==============================] - 41s 971ms/step - loss: 1.3640 - categorical_accuracy: 0.4183 - val_loss: 0.9814 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00001: saving model to model_init_1_2019-12-2313_51_49.951298/model-00001-1.36888-0.41629-0.98140-0.53000.h5\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 15s 351ms/step - loss: 1.0295 - categorical_accuracy: 0.6088 - val_loss: 2.7039 - val_categorical_accuracy: 0.4643\n",
      "\n",
      "Epoch 00002: saving model to model_init_1_2019-12-2313_51_49.951298/model-00002-1.02946-0.60884-2.70387-0.46429.h5\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 16s 374ms/step - loss: 1.0144 - categorical_accuracy: 0.6122 - val_loss: 1.0583 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to model_init_1_2019-12-2313_51_49.951298/model-00003-1.01435-0.61224-1.05828-0.50000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 13s 305ms/step - loss: 0.6818 - categorical_accuracy: 0.7618 - val_loss: 0.6429 - val_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00004: saving model to model_init_1_2019-12-2313_51_49.951298/model-00004-0.69139-0.75652-0.64290-0.71429.h5\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - 12s 277ms/step - loss: 0.7022 - categorical_accuracy: 0.7333 - val_loss: 0.9584 - val_categorical_accuracy: 0.6786\n",
      "\n",
      "Epoch 00005: saving model to model_init_1_2019-12-2313_51_49.951298/model-00005-0.70218-0.73333-0.95840-0.67857.h5\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - 13s 299ms/step - loss: 0.6558 - categorical_accuracy: 0.7333 - val_loss: 0.6458 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00006: saving model to model_init_1_2019-12-2313_51_49.951298/model-00006-0.65579-0.73333-0.64582-0.75000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - 9s 223ms/step - loss: 0.5056 - categorical_accuracy: 0.8265 - val_loss: 1.2137 - val_categorical_accuracy: 0.5714\n",
      "\n",
      "Epoch 00007: saving model to model_init_1_2019-12-2313_51_49.951298/model-00007-0.53946-0.80625-1.21371-0.57143.h5\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - 7s 164ms/step - loss: 0.4860 - categorical_accuracy: 0.8095 - val_loss: 0.5270 - val_categorical_accuracy: 0.8214\n",
      "\n",
      "Epoch 00008: saving model to model_init_1_2019-12-2313_51_49.951298/model-00008-0.48603-0.80952-0.52696-0.82143.h5\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - 7s 167ms/step - loss: 0.3718 - categorical_accuracy: 0.8492 - val_loss: 0.8412 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00009: saving model to model_init_1_2019-12-2313_51_49.951298/model-00009-0.37185-0.84921-0.84117-0.78571.h5\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - 7s 160ms/step - loss: 0.4947 - categorical_accuracy: 0.8333 - val_loss: 0.8365 - val_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00010: saving model to model_init_1_2019-12-2313_51_49.951298/model-00010-0.49469-0.83333-0.83654-0.71429.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - 8s 186ms/step - loss: 0.4505 - categorical_accuracy: 0.8175 - val_loss: 0.8216 - val_categorical_accuracy: 0.6786\n",
      "\n",
      "Epoch 00011: saving model to model_init_1_2019-12-2313_51_49.951298/model-00011-0.45049-0.81746-0.82163-0.67857.h5\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - 7s 166ms/step - loss: 0.3299 - categorical_accuracy: 0.8651 - val_loss: 0.6155 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_1_2019-12-2313_51_49.951298/model-00012-0.32993-0.86508-0.61548-0.75000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - 7s 162ms/step - loss: 0.2985 - categorical_accuracy: 0.8889 - val_loss: 0.3102 - val_categorical_accuracy: 0.9286\n",
      "\n",
      "Epoch 00013: saving model to model_init_1_2019-12-2313_51_49.951298/model-00013-0.29847-0.88889-0.31018-0.92857.h5\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - 7s 174ms/step - loss: 0.2920 - categorical_accuracy: 0.8968 - val_loss: 0.7635 - val_categorical_accuracy: 0.7857\n",
      "\n",
      "Epoch 00014: saving model to model_init_1_2019-12-2313_51_49.951298/model-00014-0.29204-0.89683-0.76348-0.78571.h5\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - 7s 177ms/step - loss: 0.2080 - categorical_accuracy: 0.9365 - val_loss: 1.0266 - val_categorical_accuracy: 0.6786\n",
      "\n",
      "Epoch 00015: saving model to model_init_1_2019-12-2313_51_49.951298/model-00015-0.20804-0.93651-1.02659-0.67857.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - 7s 163ms/step - loss: 0.2452 - categorical_accuracy: 0.8810 - val_loss: 0.4056 - val_categorical_accuracy: 0.8571\n",
      "\n",
      "Epoch 00016: saving model to model_init_1_2019-12-2313_51_49.951298/model-00016-0.24523-0.88095-0.40556-0.85714.h5\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - 7s 167ms/step - loss: 0.1868 - categorical_accuracy: 0.9603 - val_loss: 0.6559 - val_categorical_accuracy: 0.8214\n",
      "\n",
      "Epoch 00017: saving model to model_init_1_2019-12-2313_51_49.951298/model-00017-0.18677-0.96032-0.65587-0.82143.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - 7s 160ms/step - loss: 0.2007 - categorical_accuracy: 0.9524 - val_loss: 0.8622 - val_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00018: saving model to model_init_1_2019-12-2313_51_49.951298/model-00018-0.20070-0.95238-0.86224-0.71429.h5\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - 7s 164ms/step - loss: 0.2166 - categorical_accuracy: 0.9206 - val_loss: 0.5841 - val_categorical_accuracy: 0.8214\n",
      "\n",
      "Epoch 00019: saving model to model_init_1_2019-12-2313_51_49.951298/model-00019-0.21658-0.92063-0.58409-0.82143.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - 7s 166ms/step - loss: 0.1677 - categorical_accuracy: 0.9524 - val_loss: 0.3074 - val_categorical_accuracy: 0.8929\n",
      "\n",
      "Epoch 00020: saving model to model_init_1_2019-12-2313_51_49.951298/model-00020-0.16772-0.95238-0.30739-0.89286.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1e6dc1c50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n",
    "model_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the final model has a train accuracy of 95% and validation accuracy of 89%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
